# shipMARL/scripts/train_rllib_ppo_pf.py
# PPO training on MiniShip + AISCommsSim (PF tracking + GNN+LSTM policy)
#
# Phase 1-2 (shipMARL):
#   - Stage3/4 file writes are forbidden in training script/callbacks.
#   - All stage writes must go through staging/recorder.py (emit events).
#   - Identity contract: run_uuid/mode/out_dir injected by training script;
#     worker_index/vector_index derived from RLlib worker/env_index;
#     episode_uid MUST be generated by env.reset and exposed on env instance.
#
from __future__ import annotations

import os
import sys

# Auto-add project root to PYTHONPATH
_SCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))
_PROJECT_ROOT = os.path.dirname(_SCRIPT_DIR)
if _PROJECT_ROOT not in sys.path:
    sys.path.insert(0, _PROJECT_ROOT)
import json
import math
import time
import uuid
import copy
import socket
import getpass
import hashlib
import platform
import datetime
import argparse
import subprocess
import importlib
from typing import Any, Dict, Optional
from collections import deque

import numpy as np
import ray
from ray.rllib.algorithms.ppo import PPOConfig
from ray.rllib.algorithms.algorithm import Algorithm
from ray.rllib.policy.policy import PolicySpec
from ray.rllib.algorithms.callbacks import DefaultCallbacks
from ray.rllib.policy.sample_batch import SampleBatch

from miniship.wrappers.rllib_env import register_miniship_env
from miniship.models.gnn_lstm_ac import register_miniship_gnn_lstm_model

from staging.recorder import StageRecorder, StagingIdentity

# ===================== Debug env flags (kept) =====================
DEBUG_PF = bool(int(os.getenv("PF_DEBUG", "0")))
DEBUG_PF_RX_DETAIL = bool(int(os.getenv("PF_DEBUG_RX_DETAIL", "0")))
DEBUG_PF_LIFECYCLE = bool(int(os.getenv("PF_DEBUG_LIFECYCLE", "0")))
DEBUG_PF_LOG_FILE = os.getenv("PF_LOG_FILE", "")

DEBUG_PF_YAW = bool(int(os.getenv("PF_DEBUG_YAW", "0")))
_pf_yaw_sid_s = os.getenv("PF_DEBUG_YAW_SID", "").strip()
DEBUG_PF_YAW_SID = int(_pf_yaw_sid_s) if _pf_yaw_sid_s else None
DEBUG_PF_YAW_TOL = float(os.getenv("PF_DEBUG_YAW_TOL", "1e-4"))
DEBUG_PF_YAW_MAX = int(os.getenv("PF_DEBUG_YAW_MAX", "20"))

# ===================== JSON helpers =====================
def _json_default(obj):
    try:
        import numpy as _np
        if isinstance(obj, (_np.integer,)):
            return int(obj)
        if isinstance(obj, (_np.floating,)):
            return float(obj)
        if isinstance(obj, (_np.ndarray,)):
            return obj.tolist()
    except Exception:
        pass
    if hasattr(obj, "__dict__"):
        try:
            return dict(obj.__dict__)
        except Exception:
            pass
    return str(obj)

def _atomic_write_json(path: str, data: dict) -> None:
    os.makedirs(os.path.dirname(path), exist_ok=True)
    tmp = path + ".tmp"
    with open(tmp, "w", encoding="utf-8") as f:
        json.dump(data, f, ensure_ascii=False, indent=2, default=_json_default)
        f.write("\n")
    os.replace(tmp, path)

def _read_text_file(path: str, max_bytes: int = 2_000_000) -> dict:
    out = {"path": path, "ok": False, "text": None, "sha256": None, "error": None}
    try:
        if not path:
            out["error"] = "empty path"
            return out
        ap = os.path.abspath(path)
        out["path"] = ap
        with open(ap, "rb") as f:
            b = f.read(max_bytes + 1)
        if len(b) > max_bytes:
            b = b[:max_bytes]
        out["sha256"] = hashlib.sha256(b).hexdigest()
        out["text"] = b.decode("utf-8", errors="replace")
        out["ok"] = True
        return out
    except Exception as e:
        out["error"] = repr(e)
        return out

def _try_get_git_state(cwd: str) -> dict:
    st = {"ok": False, "cwd": os.path.abspath(cwd), "commit": None, "dirty": None, "error": None}
    try:
        commit = subprocess.check_output(["git", "rev-parse", "HEAD"], cwd=cwd, stderr=subprocess.STDOUT).decode().strip()
        dirty = subprocess.check_output(["git", "status", "--porcelain"], cwd=cwd, stderr=subprocess.STDOUT).decode().strip()
        st["ok"] = True
        st["commit"] = commit
        st["dirty"] = (len(dirty) > 0)
        return st
    except Exception as e:
        st["error"] = repr(e)
        return st

def _try_get_pkg_versions(pkgs) -> dict:
    out = {}
    try:
        from importlib import metadata as importlib_metadata
    except Exception:
        importlib_metadata = None
    for p in pkgs:
        v = None
        try:
            if importlib_metadata is not None:
                v = importlib_metadata.version(p)
        except Exception:
            v = None
        if v is None:
            try:
                m = importlib.import_module(p)
                v = getattr(m, "__version__", None)
            except Exception:
                v = None
        out[p] = v
    return out

def _utc_now_iso() -> str:
    return datetime.datetime.utcnow().replace(microsecond=0).isoformat() + "Z"

def _new_run_uuid() -> str:
    ts = datetime.datetime.utcnow().strftime("%Y%m%dT%H%M%SZ")
    return f"{ts}-pid{os.getpid()}-{uuid.uuid4().hex[:8]}"

# ===================== Best-effort extraction helpers =====================
def _pick_sub_env(base_env, env_index):
    if base_env is None:
        return None
    try:
        sub_envs = base_env.get_sub_environments()
    except Exception:
        sub_envs = None
    if not sub_envs:
        sub_envs = [getattr(base_env, "env", base_env)]
    if not sub_envs:
        return None
    if env_index is None or (not isinstance(env_index, int)) or env_index < 0 or env_index >= len(sub_envs):
        return sub_envs[0]
    return sub_envs[env_index]

def _unwrap_env_chain(env):
    cur = env
    seen = set()
    chain = []
    for _ in range(25):
        if cur is None:
            break
        if id(cur) in seen:
            break
        seen.add(id(cur))
        chain.append(type(cur).__name__)
        if hasattr(cur, "core_env") and getattr(cur, "core_env") is not None:
            cur = getattr(cur, "core_env")
            continue
        nxt = None
        for attr in ("unwrapped", "env", "_env", "wrapped_env", "_wrapped_env"):
            if hasattr(cur, attr):
                cand = getattr(cur, attr, None)
                if cand is not None and cand is not cur:
                    nxt = cand
                    break
        if nxt is None:
            break
        cur = nxt
    return cur, chain

def _extract_episode_params_best_effort(env):
    if env is None:
        return {}, "env=None"

    direct_names = ("episode_params", "ep_params", "ais_episode_params", "comms_episode_params")
    for n in direct_names:
        try:
            v = getattr(env, n, None)
        except Exception:
            v = None
        if v is None:
            continue
        if isinstance(v, dict):
            return v, f"env.{n}"
        if hasattr(v, "__dict__"):
            try:
                return dict(v.__dict__), f"env.{n}.__dict__"
            except Exception:
                pass

    nested_objs = ("ais", "ais_sim", "ais_comms", "ais_comms_sim", "comms", "comms_sim", "channel", "scheduler")
    for objn in nested_objs:
        try:
            obj = getattr(env, objn, None)
        except Exception:
            obj = None
        if obj is None:
            continue
        for n in direct_names:
            try:
                v = getattr(obj, n, None)
            except Exception:
                v = None
            if v is None:
                continue
            if isinstance(v, dict):
                return v, f"env.{objn}.{n}"
            if hasattr(v, "__dict__"):
                try:
                    return dict(v.__dict__), f"env.{objn}.{n}.__dict__"
                except Exception:
                    pass

        for mn in ("get_episode_params", "get_ep_params"):
            try:
                f = getattr(obj, mn, None)
            except Exception:
                f = None
            if callable(f):
                try:
                    v = f()
                except Exception:
                    v = None
                if isinstance(v, dict):
                    return v, f"env.{objn}.{mn}()"
                if v is not None and hasattr(v, "__dict__"):
                    try:
                        return dict(v.__dict__), f"env.{objn}.{mn}().__dict__"
                    except Exception:
                        pass

    try:
        cfg = getattr(env, "cfg", None)
        if isinstance(cfg, dict) and isinstance(cfg.get("episode_params", None), dict):
            return cfg["episode_params"], "env.cfg['episode_params']"
    except Exception:
        pass

    return {}, "not_found"

def _extract_obj_config_best_effort(obj) -> dict:
    if obj is None:
        return {}
    f = getattr(obj, "get_config", None)
    if callable(f):
        try:
            d = f()
            if isinstance(d, dict):
                return d
        except Exception:
            pass
    for attr in ("cfg", "config", "env_config", "_cfg", "_config"):
        try:
            d = getattr(obj, attr, None)
            if isinstance(d, dict):
                return d
        except Exception:
            pass
    return {}

def _extract_trackmgr_cfg_best_effort(env):
    if env is None:
        return {}
    tm = getattr(env, "track_mgr", None)
    if tm is None:
        return {}
    snap = {"track_mgr_class": tm.__class__.__name__, "track_mgr_module": tm.__class__.__module__}
    try:
        snap["track_mgr_cfg"] = _extract_obj_config_best_effort(tm)
    except Exception:
        snap["track_mgr_cfg"] = {}
    return snap

def _get_env_episode_uid_strict(real_env, strict: bool) -> str:
    """
    Phase-2 contract:
      env.reset MUST generate episode_uid and expose it on env instance.
    """
    cand_names = ("episode_uid", "current_episode_uid", "_episode_uid", "ep_uid", "episode_id_uid")
    for n in cand_names:
        try:
            v = getattr(real_env, n, None)
        except Exception:
            v = None
        if isinstance(v, str) and v.strip():
            return v.strip()
    if strict:
        raise RuntimeError(
            "Identity contract violation: env does not expose episode_uid. "
            "You must implement env.reset to generate episode_uid and set env.episode_uid (or current_episode_uid)."
        )
    return ""

# ===================== Wilson CI =====================
def wilson_ci(k: int, n: int, conf: float = 0.95):
    if n <= 0:
        return (0.0, 1.0)
    _z = {
        0.90: 1.6448536269514722,
        0.95: 1.959963984540054,
        0.975: 2.241402727604947,
        0.99: 2.5758293035489004,
    }.get(conf, 1.959963984540054)

    p = k / n
    z2 = _z * _z
    denom = 1.0 + z2 / n
    center = p + z2 / (2.0 * n)
    margin = _z * math.sqrt((p * (1.0 - p) / n) + (z2 / (4.0 * n * n)))
    lo = max(0.0, (center - margin) / denom)
    hi = min(1.0, (center + margin) / denom)
    return lo, hi

# ===================== Callbacks (single consolidated) =====================
class MiniShipCallbacks(DefaultCallbacks):
    """
    - Reward shaping with Lagrangian (near/rule)
    - Dual sync
    - Episode terminal stats (succ/coll/tout)
    - STAGING: emit stage3/stage4 records through StageRecorder (no direct stage writes)
    """
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)

        # Register custom model in each worker (needed for Ray worker processes)
        try:
            register_miniship_gnn_lstm_model()
        except Exception:
            pass  # Already registered or import issue

        # reward shaping knobs (default; overridden by env_cfg)
        self.step_cost = 0.03
        self.reward_scale = 1.0
        self.use_lagrangian = False

        self.lam = {"near": 3.0, "rule": 2.0}
        self.dual_eta = {"near": 0.005, "rule": 0.005}
        self.ctarget = {"near": 0.04, "rule": 0.04}
        self.dual_beta = 0.97
        self.lam_max = {"near": 5.0, "rule": 5.0}
        self.dual_update_every = 2

        self._dual_version = 0
        self._dual_env_push_version = -1
        self._algo_cfg_initialized = False
        self.running_c = {k: 0.0 for k in self.lam.keys()}

        # freeze knobs
        self.dual_freeze_on_success = True
        self.dual_freeze_patience = 3
        self.dual_freeze_succ = 0.95
        self.dual_freeze_coll = 0.04
        self.dual_freeze_tout = 0.03
        self._dual_freeze_hits = 0
        self._dual_frozen = False
        self.dual_freeze = False  # manual freeze flag (env_cfg can set)

        # STAGING recorder cache: key=(mode, wid, v) -> recorder
        self._staging_recorders: Dict[tuple, StageRecorder] = {}
        self._staging_out_dir: Optional[str] = None
        self._staging_strict: bool = True

        # init latch from env_cfg
        self._cfg_initialized = False

    # ---------------- dual snapshot helpers ----------------
    def get_dual_snapshot(self) -> dict:
        return {
            "dual_version": int(self._dual_version),
            "lam": {k: float(v) for k, v in self.lam.items()},
            "running_c": {k: float(v) for k, v in self.running_c.items()},
        }

    def set_dual_snapshot(self, snap: dict) -> None:
        if not isinstance(snap, dict):
            return
        lam = snap.get("lam", {})
        running_c = snap.get("running_c", {})
        if isinstance(lam, dict):
            for k in self.lam.keys():
                if k in lam:
                    self.lam[k] = float(lam[k])
        if isinstance(running_c, dict):
            for k in self.running_c.keys():
                if k in running_c:
                    self.running_c[k] = float(running_c[k])
        if "dual_version" in snap:
            self._dual_version = int(snap["dual_version"])

    def _maybe_init_from_env_cfg(self, base_env):
        if self._cfg_initialized:
            return

        sub = _pick_sub_env(base_env, 0)
        real_env, _ = _unwrap_env_chain(sub)
        env_cfg = getattr(real_env, "cfg", None)
        if not isinstance(env_cfg, dict):
            env_cfg = {}

        def _to_bool(v, default: bool) -> bool:
            if isinstance(v, bool):
                return v
            if isinstance(v, (int, float, np.integer, np.floating)):
                try:
                    return bool(int(v))
                except Exception:
                    return bool(v)
            if isinstance(v, str):
                s = v.strip().lower()
                if s in ("1", "true", "t", "yes", "y", "on"):
                    return True
                if s in ("0", "false", "f", "no", "n", "off", "none", "null", ""):
                    return False
            return default

        self.step_cost = float(env_cfg.get("step_cost", self.step_cost))
        self.reward_scale = float(env_cfg.get("reward_scale", self.reward_scale))
        self.use_lagrangian = _to_bool(env_cfg.get("use_lagrangian", self.use_lagrangian), self.use_lagrangian)

        lam_init = env_cfg.get("lambda_init", {})
        if isinstance(lam_init, dict):
            for k in self.lam.keys():
                if k in lam_init:
                    self.lam[k] = float(lam_init[k])

        dual_eta = env_cfg.get("dual_eta", {})
        if isinstance(dual_eta, dict):
            for k in self.dual_eta.keys():
                if k in dual_eta:
                    self.dual_eta[k] = float(dual_eta[k])

        self.dual_beta = float(env_cfg.get("dual_beta", self.dual_beta))

        ctarget = env_cfg.get("ctarget", {})
        if isinstance(ctarget, dict):
            for k in self.ctarget.keys():
                if k in ctarget:
                    self.ctarget[k] = float(ctarget[k])

        lam_max = env_cfg.get("lambda_max", None)
        if isinstance(lam_max, dict):
            for k in self.lam_max.keys():
                if k in lam_max:
                    self.lam_max[k] = float(lam_max[k])

        try:
            self.dual_update_every = int(env_cfg.get("dual_update_every", self.dual_update_every) or self.dual_update_every)
        except Exception:
            pass

        self.dual_freeze = _to_bool(env_cfg.get("dual_freeze", self.dual_freeze), self.dual_freeze)
        self.dual_freeze_on_success = _to_bool(env_cfg.get("dual_freeze_on_success", self.dual_freeze_on_success), self.dual_freeze_on_success)
        self.dual_freeze_patience = int(env_cfg.get("dual_freeze_patience", self.dual_freeze_patience) or self.dual_freeze_patience)
        self.dual_freeze_succ = float(env_cfg.get("dual_freeze_succ", self.dual_freeze_succ))
        self.dual_freeze_coll = float(env_cfg.get("dual_freeze_coll", self.dual_freeze_coll))
        self.dual_freeze_tout = float(env_cfg.get("dual_freeze_tout", self.dual_freeze_tout))

        # STAGING config is passed via env_cfg["staging"]
        st = env_cfg.get("staging", None)
        if isinstance(st, dict):
            out_dir = str(st.get("out_dir", env_cfg.get("out_dir", "")) or "")
            self._staging_out_dir = os.path.abspath(out_dir) if out_dir else os.path.abspath(env_cfg.get("out_dir", "."))
            self._staging_strict = bool(st.get("strict", True))
        else:
            # default: still require staging in Phase1-2
            self._staging_out_dir = os.path.abspath(env_cfg.get("out_dir", "."))
            self._staging_strict = True

        self._cfg_initialized = True

        print(
            "[shipMARL][CallbackCfg] init_ok "
            f"use_lagrangian={self.use_lagrangian} lam={self.lam} "
            f"staging_out={self._staging_out_dir} strict={self._staging_strict}"
        )

    @staticmethod
    def _infer_mode(env_cfg: dict) -> str:
        """Infer mode (train/eval) from env config."""
        mode = env_cfg.get("mode", "train")
        if isinstance(mode, str) and mode.strip():
            return mode.strip()
        return "train"

    def _get_recorder(self, *, env_cfg: dict, worker_index: int, vector_index: int) -> Optional[StageRecorder]:
        if self._staging_out_dir is None:
            # should have been set in _maybe_init_from_env_cfg; fallback
            self._staging_out_dir = os.path.abspath(env_cfg.get("out_dir", "."))
            self._staging_strict = True

        run_uuid = str(env_cfg.get("run_uuid", "") or "")
        mode = self._infer_mode(env_cfg)

        key = (mode, int(worker_index), int(vector_index))
        rec = self._staging_recorders.get(key, None)
        if rec is not None:
            return rec

        if not run_uuid:
            # Skip staging if run_uuid not available (e.g., in probe/test mode)
            return None

        ident = StagingIdentity(
            run_uuid=run_uuid,
            mode=mode,
            out_dir=self._staging_out_dir,
            worker_index=int(worker_index),
            vector_index=int(vector_index),
        )
        rec = StageRecorder(ident)
        self._staging_recorders[key] = rec
        return rec

    # ---------------- episode lifecycle ----------------
    def on_episode_start(self, *, worker, base_env, policies, episode, env_index=None, **kwargs):
        self._maybe_init_from_env_cfg(base_env)

        # episode accumulators
        episode.user_data["R_shaped"] = 0.0
        episode.user_data["steps"] = 0
        episode.user_data["R_env"] = 0.0
        episode.user_data["R_base"] = 0.0
        for k in self.lam.keys():
            episode.user_data[f"{k}_sum"] = 0.0

        # STAGING emit stage3 ctx (reset-time context)
        sub_env = _pick_sub_env(base_env, env_index)
        real_env, env_chain = _unwrap_env_chain(sub_env)

        env_cfg = getattr(real_env, "cfg", None)
        if not isinstance(env_cfg, dict):
            env_cfg = {}

        try:
            wid = int(getattr(worker, "worker_index", -1))
        except Exception:
            wid = -1
        try:
            vid = int(env_index if env_index is not None else 0)
        except Exception:
            vid = 0

        rec = self._get_recorder(env_cfg=env_cfg, worker_index=wid, vector_index=vid)

        # Skip staging if no recorder (missing run_uuid)
        if rec is None:
            # Still try to get episode_uid non-strictly for metrics
            episode_uid = _get_env_episode_uid_strict(real_env, strict=False)
            episode.user_data["episode_uid"] = episode_uid
            return

        # Phase2 strict: episode_uid must come from env.reset and be exposed here
        # Only enforce strict mode when recorder is available
        episode_uid = _get_env_episode_uid_strict(real_env, strict=self._staging_strict)
        episode.user_data["episode_uid"] = episode_uid  # keep for end

        ep_params, ep_params_src = _extract_episode_params_best_effort(real_env)
        tm_snap = _extract_trackmgr_cfg_best_effort(real_env)

        # Build env_config for stage3 recording
        stage3_env_config = {
            "N": env_cfg.get("N", 2),
            "dt": env_cfg.get("dt", 0.5),
            "T_max": env_cfg.get("T_max", 100.0),
            "ais_cfg_path": str(env_cfg.get("ais_cfg_path", "")),
            "ais_cfg_sha256": str(env_cfg.get("ais_cfg_sha256", "")),
        }

        # Build ais_params for stage3 recording
        stage3_ais_params = {
            "episode_params": ep_params,
            "episode_params_src": ep_params_src,
        }

        # Extra metadata
        extra = {
            "started_utc": _utc_now_iso(),
            "env_chain": env_chain,
            "track_mgr_snapshot": tm_snap,
            "identity": {
                "run_uuid": str(env_cfg.get("run_uuid", "")),
                "mode": self._infer_mode(env_cfg),
                "out_dir": str(env_cfg.get("out_dir", "")),
                "worker_index": wid,
                "vector_index": vid,
            },
        }

        rec.emit_stage3_episode_init(
            episode_uid=episode_uid,
            episode_idx=int(getattr(episode, "episode_id", -1) or -1),
            env_config=stage3_env_config,
            ais_params=stage3_ais_params,
            ship_init_states=[],  # Will be populated by env if needed
            extra=extra,
        )

    def on_postprocess_trajectory(
        self,
        *,
        worker,
        episode,
        agent_id,
        policy_id,
        policies,
        postprocessed_batch,
        original_batches,
        **kwargs,
    ):
        if SampleBatch.REWARDS not in postprocessed_batch:
            return

        raw_env_rewards = np.asarray(postprocessed_batch[SampleBatch.REWARDS], dtype=np.float32)
        infos = postprocessed_batch.get(SampleBatch.INFOS, None)
        if infos is None:
            infos = [{} for _ in range(len(raw_env_rewards))]
        else:
            infos = list(infos)
            if len(infos) != len(raw_env_rewards):
                infos = (infos + [{}] * len(raw_env_rewards))[: len(raw_env_rewards)]

        episode.user_data["R_env"] += float(np.sum(raw_env_rewards))

        def _unwrap_info(x: Any) -> Dict[str, Any]:
            if not isinstance(x, dict):
                return {}
            # common: {agent_id: {...}}
            if agent_id in x and isinstance(x.get(agent_id), dict):
                return x[agent_id]
            # already agent info
            if ("c_near" in x) or ("c_rule" in x) or ("term_reason" in x) or ("terminal_reason" in x):
                return x
            # single nest
            if len(x) == 1:
                v = next(iter(x.values()))
                if isinstance(v, dict):
                    return v
            return x

        # base reward selection:
        base_rewards = raw_env_rewards.copy()
        base_src = "env"
        recovered_any = False
        if np.allclose(base_rewards, 0.0):
            cand_keys = ("r_task", "task_reward", "reward_task", "r_env", "env_reward", "reward_env", "reward", "rew", "r")
            tmp = np.zeros_like(base_rewards, dtype=np.float32)
            for t, raw in enumerate(infos):
                info = _unwrap_info(raw)
                for k in cand_keys:
                    v = info.get(k, None)
                    if isinstance(v, (int, float, np.floating, np.integer)):
                        tmp[t] = float(v)
                        recovered_any = True
                        break
            if recovered_any:
                base_rewards = tmp
                base_src = "info.reward"
            else:
                if self.step_cost > 0:
                    base_rewards = (-float(self.step_cost)) * np.ones_like(base_rewards, dtype=np.float32)
                    base_src = "fallback.-step_cost"

        if base_src != "env":
            base_rewards = base_rewards * float(self.reward_scale)

        shaped_rewards = base_rewards.copy()
        c_sums_local = {k: 0.0 for k in self.lam.keys()}

        for t, raw in enumerate(infos):
            info = _unwrap_info(raw)
            c_near = max(0.0, float(info.get("c_near", 0.0) or 0.0))
            c_rule = max(0.0, float(info.get("c_rule", 0.0) or 0.0))
            c_sums_local["near"] += c_near
            c_sums_local["rule"] += c_rule
            if self.use_lagrangian:
                shaped_rewards[t] -= float(self.lam["near"]) * c_near
                shaped_rewards[t] -= float(self.lam["rule"]) * c_rule

        postprocessed_batch[SampleBatch.REWARDS] = shaped_rewards.astype(np.float32)

        episode.user_data["R_base"] += float(np.sum(base_rewards))
        episode.user_data["R_shaped"] += float(np.sum(shaped_rewards))
        episode.user_data["steps"] += int(len(shaped_rewards))
        for k, v in c_sums_local.items():
            episode.user_data[f"{k}_sum"] += float(v)

    def on_episode_end(self, *, worker, base_env, policies, episode, env_index=None, **kwargs):
        steps = max(1, int(episode.user_data.get("steps", 1)))
        c_mean = {}
        for k in self.lam.keys():
            c_total = float(episode.user_data.get(f"{k}_sum", 0.0) or 0.0)
            c_mean[k] = c_total / steps

        # termstats: prefer core_env._last_infos; fallback to last_info_for
        sub_env = _pick_sub_env(base_env, env_index)
        real_env, _ = _unwrap_env_chain(sub_env)
        core_env = getattr(real_env, "core_env", None) or real_env
        core_infos = getattr(core_env, "_last_infos", None)

        succ_cnt = coll_cnt = tout_cnt = other_cnt = 0
        agent_ids = []

        def _classify_reason(r: str):
            r = (r or "").lower().strip()
            if r in ("collision", "coll", "crash", "collide"):
                return "coll"
            if r in ("success", "succ", "arrival", "arrive", "goal"):
                return "succ"
            if r in ("timeout", "time_out", "tout", "timeup", "time_up"):
                return "tout"
            return "other"

        any_reason = False
        if isinstance(core_infos, dict):
            agent_ids = [aid for aid in core_infos.keys() if aid not in ("__all__", "__common__")]
            for aid in agent_ids:
                info = core_infos.get(aid, None)
                if isinstance(info, dict):
                    r = info.get("term_reason") or info.get("terminal_reason") or info.get("termination_reason")
                    if isinstance(r, str) and r.strip():
                        any_reason = True
                        break

        if any_reason and agent_ids:
            for aid in agent_ids:
                info = core_infos.get(aid, {}) if isinstance(core_infos.get(aid, {}), dict) else {}
                cat = _classify_reason(info.get("term_reason") or info.get("terminal_reason") or info.get("termination_reason") or "")
                if cat == "succ":
                    succ_cnt += 1
                elif cat == "coll":
                    coll_cnt += 1
                elif cat == "tout":
                    tout_cnt += 1
                else:
                    other_cnt += 1
            n = len(agent_ids)
            termstats_valid = 1
        else:
            # fallback: use RLlib last_info_for
            try:
                aids = list(episode.get_agents())
            except Exception:
                aids = list(getattr(episode, "_agent_to_policy", {}).keys())
            agent_ids = [a for a in aids if a not in ("__all__", "__common__")]
            n = len(agent_ids)
            succ_cnt = coll_cnt = tout_cnt = other_cnt = 0
            for aid in agent_ids:
                try:
                    info = episode.last_info_for(aid)
                except Exception:
                    info = None
                if isinstance(info, dict) and aid in info and isinstance(info.get(aid), dict):
                    info = info[aid]
                if not isinstance(info, dict):
                    continue
                r = info.get("term_reason") or info.get("terminal_reason") or info.get("termination_reason")
                cat = _classify_reason(r or ("timeout" if (info.get("timeout") or info.get("time_out")) else ""))
                if cat == "succ":
                    succ_cnt += 1
                elif cat == "coll":
                    coll_cnt += 1
                elif cat == "tout":
                    tout_cnt += 1
                else:
                    # unknown -> timeout (to keep Wilson n consistent)
                    tout_cnt += 1
            termstats_valid = 1 if n > 0 else 0

        # episode-level bins
        if n <= 0:
            succ_ep_bin = 0
            coll_ep_bin = 0
            tout_ep_bin = 1
        else:
            coll_ep_bin = 1 if coll_cnt > 0 else 0
            succ_ep_bin = 1 if (succ_cnt == n and coll_cnt == 0) else 0
            tout_ep_bin = 1 if (coll_ep_bin == 0 and succ_ep_bin == 0) else 0

        # write custom_metrics
        episode.custom_metrics["termstats_valid"] = float(termstats_valid)
        episode.custom_metrics["succ_rate"] = (succ_cnt / n) if n > 0 else 0.0
        episode.custom_metrics["coll_rate"] = (coll_cnt / n) if n > 0 else 0.0
        episode.custom_metrics["tout_rate"] = (tout_cnt / n) if n > 0 else 1.0

        episode.custom_metrics["succ_ep_bin"] = float(succ_ep_bin)
        episode.custom_metrics["coll_ep_bin"] = float(coll_ep_bin)
        episode.custom_metrics["tout_ep_bin"] = float(tout_ep_bin)

        episode.custom_metrics["policy_env_reward"] = float(episode.user_data.get("R_env", 0.0) or 0.0)
        episode.custom_metrics["policy_base_reward"] = float(episode.user_data.get("R_base", 0.0) or 0.0)
        episode.custom_metrics["policy_shaped_reward"] = float(episode.user_data.get("R_shaped", 0.0) or 0.0)

        episode.custom_metrics["c_near"] = float(c_mean["near"])
        episode.custom_metrics["c_rule"] = float(c_mean["rule"])

        episode.custom_metrics["dual_version_worker"] = float(getattr(self, "_dual_version", -1))
        episode.custom_metrics["lam_near_worker"] = float(self.lam["near"])
        episode.custom_metrics["lam_rule_worker"] = float(self.lam["rule"])

        # STAGING emit stage4 summary (no direct stage file writes)
        sub_env = _pick_sub_env(base_env, env_index)
        real_env, env_chain = _unwrap_env_chain(sub_env)
        env_cfg = getattr(real_env, "cfg", None)
        if not isinstance(env_cfg, dict):
            env_cfg = {}

        try:
            wid = int(getattr(worker, "worker_index", -1))
        except Exception:
            wid = -1
        try:
            vid = int(env_index if env_index is not None else 0)
        except Exception:
            vid = 0

        rec = self._get_recorder(env_cfg=env_cfg, worker_index=wid, vector_index=vid)

        # Skip staging if no recorder (missing run_uuid in worker)
        if rec is None:
            return

        episode_uid = str(episode.user_data.get("episode_uid", "") or "")
        if self._staging_strict and not episode_uid:
            # strict: must exist, never drift
            episode_uid = _get_env_episode_uid_strict(real_env, strict=True)

        payload = {
            "ended_utc": _utc_now_iso(),
            "env_chain": env_chain,
            "episode": {
                "episode_id": int(getattr(episode, "episode_id", -1) or -1),
                "len_steps": int(getattr(episode, "length", 0) or 0),
            },
            "outcome": {
                "termstats_valid": float(episode.custom_metrics.get("termstats_valid", 0.0) or 0.0),
                "succ_ep_bin": float(episode.custom_metrics.get("succ_ep_bin", 0.0) or 0.0),
                "coll_ep_bin": float(episode.custom_metrics.get("coll_ep_bin", 0.0) or 0.0),
                "tout_ep_bin": float(episode.custom_metrics.get("tout_ep_bin", 0.0) or 0.0),
                "succ_rate": float(episode.custom_metrics.get("succ_rate", 0.0) or 0.0),
                "coll_rate": float(episode.custom_metrics.get("coll_rate", 0.0) or 0.0),
                "tout_rate": float(episode.custom_metrics.get("tout_rate", 0.0) or 0.0),
            },
            "reward": {
                "policy_env_reward": float(episode.custom_metrics.get("policy_env_reward", 0.0) or 0.0),
                "policy_base_reward": float(episode.custom_metrics.get("policy_base_reward", 0.0) or 0.0),
                "policy_shaped_reward": float(episode.custom_metrics.get("policy_shaped_reward", 0.0) or 0.0),
            },
            "constraints": {
                "c_near": float(episode.custom_metrics.get("c_near", 0.0) or 0.0),
                "c_rule": float(episode.custom_metrics.get("c_rule", 0.0) or 0.0),
            },
            "dual": {
                "lam_near": float(self.lam["near"]),
                "lam_rule": float(self.lam["rule"]),
                "dual_version": int(self._dual_version),
            },
        }

        # Determine termination reason
        term_reason = "unknown"
        if episode.custom_metrics.get("coll_ep_bin", 0.0) > 0:
            term_reason = "collision"
        elif episode.custom_metrics.get("succ_ep_bin", 0.0) > 0:
            term_reason = "success"
        elif episode.custom_metrics.get("tout_ep_bin", 0.0) > 0:
            term_reason = "timeout"

        rec.emit_stage4_episode_end(
            episode_uid=episode_uid,
            episode_idx=int(getattr(episode, "episode_id", -1) or -1),
            t_end=float(getattr(episode, "length", 0) or 0) * float(env_cfg.get("dt", 0.5)),
            term_reason=term_reason,
            stats=payload,
        )

    def on_train_result(self, *, algorithm, result, **kwargs):
        """
        Dual variable update after each training iteration.
        """
        if self._dual_frozen or self.dual_freeze:
            return

        # Get episode stats from result
        ep_succ = result.get("custom_metrics", {}).get("succ_ep_bin_mean", 0.0) or 0.0
        ep_coll = result.get("custom_metrics", {}).get("coll_ep_bin_mean", 0.0) or 0.0
        ep_tout = result.get("custom_metrics", {}).get("tout_ep_bin_mean", 0.0) or 0.0

        c_near = result.get("custom_metrics", {}).get("c_near_mean", 0.0) or 0.0
        c_rule = result.get("custom_metrics", {}).get("c_rule_mean", 0.0) or 0.0

        # Update running estimates
        self.running_c["near"] = self.dual_beta * self.running_c["near"] + (1 - self.dual_beta) * c_near
        self.running_c["rule"] = self.dual_beta * self.running_c["rule"] + (1 - self.dual_beta) * c_rule

        # Dual update (only every N iterations)
        iter_num = result.get("training_iteration", 0)
        if iter_num % self.dual_update_every == 0 and self.use_lagrangian:
            for k in self.lam.keys():
                grad = self.running_c[k] - self.ctarget[k]
                self.lam[k] = max(0.0, min(self.lam_max[k], self.lam[k] + self.dual_eta[k] * grad))
            self._dual_version += 1

        # Auto freeze check
        if self.dual_freeze_on_success:
            if ep_succ >= self.dual_freeze_succ and ep_coll <= self.dual_freeze_coll and ep_tout <= self.dual_freeze_tout:
                self._dual_freeze_hits += 1
                if self._dual_freeze_hits >= self.dual_freeze_patience:
                    self._dual_frozen = True
                    print(f"[shipMARL][Dual] Frozen at iter {iter_num}: succ={ep_succ:.3f} coll={ep_coll:.3f}")
            else:
                self._dual_freeze_hits = 0


# ===================== Main training function =====================
def build_ppo_config(args, run_uuid: str) -> PPOConfig:
    """Build PPO configuration for MiniShip training."""

    # Build env config first (needed for registration)
    env_cfg = {
        "N": args.N,
        "dt": args.dt,
        "T_max": args.T_max,
        "use_lagrangian": args.use_lagrangian,
        "use_guard": args.use_guard,
        "use_ais_obs": args.use_ais_obs,
        "ais_cfg_path": args.ais_cfg_path,
        "mode": "train",
        "out_dir": args.out_dir,
        "run_uuid": run_uuid,
        "staging_enable": args.staging_enable,
        "staging_record_steps": args.staging_record_steps,
        "staging_record_pf": args.staging_record_pf,
        "staging": {
            "out_dir": args.out_dir,
            "strict": True,
        },
    }

    # Register environment and model
    env_name, obs_space, act_space = register_miniship_env(env_cfg)
    register_miniship_gnn_lstm_model()

    config = (
        PPOConfig()
        .environment(
            env=env_name,
            env_config=env_cfg,
            disable_env_checking=True,  # Wrapper has custom action_space_sample
        )
        .framework("torch")
        .training(
            lr=args.lr,
            train_batch_size=args.train_batch_size,
            sgd_minibatch_size=args.sgd_minibatch_size,
            num_sgd_iter=args.num_sgd_iter,
            entropy_coeff=args.entropy_coeff,
            gamma=0.99,
            lambda_=0.95,
            clip_param=0.2,
            vf_clip_param=10.0,
        )
        .rollouts(
            num_rollout_workers=args.num_workers,
            num_envs_per_worker=args.num_envs_per_worker,
            rollout_fragment_length="auto",
        )
        .callbacks(MiniShipCallbacks)
        .multi_agent(
            policies={
                "shared_policy": PolicySpec(),
            },
            policy_mapping_fn=lambda agent_id, episode, worker, **kwargs: "shared_policy",
        )
    )

    if args.model == "gnn_lstm":
        # Model is registered in MiniShipCallbacks.__init__ for each worker
        print("[shipMARL] Using GNN-LSTM model")
        config = config.training(
            model={
                "custom_model": "miniship_gnn_lstm_ac",
                "custom_model_config": {
                    "gnn_hidden_size": 128,
                    "lstm_hidden_size": 128,
                    "num_neighbors": 6,
                    "neighbor_dim": 11,
                    "edge_dim": 8,
                    "id_dim": 1,
                },
                "max_seq_len": 64,
            }
        )

    return config


def main():
    parser = argparse.ArgumentParser(description="Train PPO on MiniShip with AIS/PF")

    # Environment
    parser.add_argument("--N", type=int, default=2, help="Number of ships")
    parser.add_argument("--dt", type=float, default=0.5, help="Simulation timestep")
    parser.add_argument("--T_max", type=float, default=100.0, help="Max episode time")
    parser.add_argument("--use-lagrangian", action="store_true", help="Use Lagrangian reward shaping")
    parser.add_argument("--use-guard", action="store_true", help="Use safety guard")
    parser.add_argument("--use-ais-obs", action="store_true", help="Use AIS observations")
    parser.add_argument("--ais-cfg-path", default="ais_comms/ais_config_pf.yaml", help="AIS config path")

    # Training
    parser.add_argument("--lr", type=float, default=3e-4, help="Learning rate")
    parser.add_argument("--train-batch-size", type=int, default=4000, help="Train batch size")
    parser.add_argument("--sgd-minibatch-size", type=int, default=256, help="SGD minibatch size")
    parser.add_argument("--num-sgd-iter", type=int, default=10, help="Number of SGD iterations")
    parser.add_argument("--entropy-coeff", type=float, default=0.01, help="Entropy coefficient")
    parser.add_argument("--num-workers", type=int, default=4, help="Number of rollout workers")
    parser.add_argument("--num-envs-per-worker", type=int, default=2, help="Envs per worker")
    parser.add_argument("--model", default="gnn_lstm", choices=["gnn_lstm", "mlp"], help="Model type")

    # Staging
    parser.add_argument("--out-dir", default="training_output", help="Output directory")
    parser.add_argument("--staging-enable", action="store_true", help="Enable staging recording")
    parser.add_argument("--staging-record-steps", action="store_true", help="Record step-level data")
    parser.add_argument("--staging-record-pf", action="store_true", help="Record PF estimates")

    # Run control
    parser.add_argument("--iterations", type=int, default=100, help="Training iterations")
    parser.add_argument("--checkpoint-freq", type=int, default=10, help="Checkpoint frequency")
    parser.add_argument("--resume", default=None, help="Resume from checkpoint")

    args = parser.parse_args()

    # Initialize Ray
    ray.init(ignore_reinit_error=True)

    run_uuid = _new_run_uuid()
    print(f"[shipMARL] Starting run: {run_uuid}")
    print(f"[shipMARL] Output: {args.out_dir}")

    # Create output directory
    os.makedirs(args.out_dir, exist_ok=True)

    # Save run config
    run_cfg = {
        "run_uuid": run_uuid,
        "started_utc": _utc_now_iso(),
        "args": vars(args),
        "git": _try_get_git_state(os.getcwd()),
        "packages": _try_get_pkg_versions(["ray", "torch", "numpy", "gymnasium"]),
        "host": {
            "hostname": socket.gethostname(),
            "user": getpass.getuser(),
            "platform": platform.platform(),
            "python": sys.version,
        },
    }
    _atomic_write_json(os.path.join(args.out_dir, "run_config.json"), run_cfg)

    # Build and train
    config = build_ppo_config(args, run_uuid)
    algo = config.build()

    if args.resume:
        print(f"[shipMARL] Resuming from: {args.resume}")
        algo.restore(args.resume)

    best_reward = float("-inf")

    for i in range(1, args.iterations + 1):
        result = algo.train()

        ep_reward = result.get("episode_reward_mean", 0.0) or 0.0
        succ_rate = result.get("custom_metrics", {}).get("succ_ep_bin_mean", 0.0) or 0.0
        coll_rate = result.get("custom_metrics", {}).get("coll_ep_bin_mean", 0.0) or 0.0

        print(f"[iter {i:4d}] reward={ep_reward:+.2f} succ={succ_rate:.2%} coll={coll_rate:.2%}")

        # Checkpoint
        if i % args.checkpoint_freq == 0:
            ckpt = algo.save(os.path.join(args.out_dir, "checkpoints"))
            print(f"[shipMARL] Checkpoint saved: {ckpt}")

        # Best model
        if ep_reward > best_reward:
            best_reward = ep_reward
            algo.save(os.path.join(args.out_dir, "best"))

    # Final save
    final_ckpt = algo.save(os.path.join(args.out_dir, "final"))
    print(f"[shipMARL] Training complete. Final checkpoint: {final_ckpt}")

    algo.stop()
    ray.shutdown()

    return 0


if __name__ == "__main__":
    sys.exit(main())