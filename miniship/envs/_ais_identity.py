from __future__ import annotations

import hashlib
from dataclasses import dataclass
from typing import Any, Dict


@dataclass(frozen=True, slots=True)
class RunIdentity:
    """
    Run identity injected by RLlib training script (Phase 2).
    This is duplicated (intentionally) in env space to avoid importing staging in core logic.
    """
    run_uuid: str
    out_dir: str
    mode: str
    worker_index: int
    vector_index: int

    @staticmethod
    def from_cfg(cfg: Dict[str, Any]) -> "RunIdentity":
        missing = [k for k in ("run_uuid", "out_dir", "mode", "worker_index", "vector_index") if k not in cfg]
        if missing:
            raise KeyError(f"[RunIdentity] missing keys in cfg: {missing}")
        mode = str(cfg["mode"])
        if mode not in ("train", "eval"):
            raise ValueError(f"[RunIdentity] invalid mode={mode!r}")
        wi = int(cfg["worker_index"])
        vi = int(cfg["vector_index"])
        if wi < 0 or vi < 0:
            raise ValueError(f"[RunIdentity] invalid worker/vector: {wi},{vi}")
        return RunIdentity(
            run_uuid=str(cfg["run_uuid"]),
            out_dir=str(cfg["out_dir"]),
            mode=mode,
            worker_index=wi,
            vector_index=vi,
        )

    def to_dict(self) -> Dict[str, Any]:
        return {
            "run_uuid": self.run_uuid,
            "out_dir": self.out_dir,
            "mode": self.mode,
            "worker_index": self.worker_index,
            "vector_index": self.vector_index,
        }


@dataclass(frozen=True, slots=True)
class EpisodeIdentity:
    """
    Episode identity (Phase 2): generated by env.reset and MUST stay stable for the entire episode.
    """
    episode_uid: str
    episode_idx: int

    def to_dict(self) -> Dict[str, Any]:
        return {"episode_uid": self.episode_uid, "episode_idx": self.episode_idx}


class EpisodeIdAllocator:
    """
    Deterministic episode_uid allocator to avoid drift and facilitate validation.

    episode_uid = sha1(run_uuid | mode | worker | vector | episode_idx).hexdigest()[:16]

    This guarantees uniqueness across worker/vector shards for a given run_uuid, as long as
    episode_idx does not repeat. If a process restarts and repeats episode_idx, StageRecorder
    MUST raise (no silent resume).
    """

    def __init__(self, run: RunIdentity):
        self._run = run
        self._episode_idx = -1

    def next(self) -> EpisodeIdentity:
        self._episode_idx += 1
        s = f"{self._run.run_uuid}|{self._run.mode}|{self._run.worker_index}|{self._run.vector_index}|{self._episode_idx}"
        uid = hashlib.sha1(s.encode("utf-8")).hexdigest()[:16]
        return EpisodeIdentity(episode_uid=uid, episode_idx=self._episode_idx)
